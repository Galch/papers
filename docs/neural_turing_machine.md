---
layout: page
group: "neural_turing_machine"
title: "Neural Turing Machine"
link_url: https://arxiv.org/abs/1410.5401
---

### 튜링 머신 (Turing Machine)

- 튜링 머신(*Turing machine*)은 긴 테이프에 쓰여있는 여러 가지 기호들을 일정한 규칙에 따라 바꾸는 기계를 의미.
- 엘런 튜링이 제안한 머신으로 현대 컴퓨터의 초안이 됨.
- 이 논문은 뉴럴 튜링 머신 (*Neural Turing Machine*) 을 다루고 있기 때문에 당연히 먼저 튜링 머신을 알아야 한다.

### 튜링 머신의 이해

- 튜링 머신은 "계산 가능한 수에 대해, 수리 명제 자동 생성 문제의 응용" 이란 엘런 튜링의 논문에서 언급되었다.
- 이 논문이 나요게 된 배경부터 살펴보자.
- 1928년. 당시 수학계를 이끌던 *다비트 힐베르트* (*David Hilbert*) 는 다음과 같은 의문을 제시하였다.
    - 수학자들이 지금까지 해 왔던 일들을 정리해보니 몇 가지 추론 법칙을 조합하여 원하는 결과를 도출하는 것이 전부임.
    - 그렇다면 몇 개의 추론 규칙을 제시하면 앞으로 수학자들이 증명할 명제 또한 모두 찾을 수 있지 않을까?
    - 즉, 모든 수리 명제를 자동으로 만들어 낼 날이 올 수도 있겠구나.
    - 힐베르트는 이러한 보편 규칙이 존재하는가에 대한 의문을 제시했다.
- 하지만 3년 후 독일의 신참 수학자 쿠르트 괴텔이 이는 불가능한 일임을 증명해버렸다.
    - "기계적인 방식만으로는 수학의 모든 사실을 구성할 수 없다."
    - 이를 불완전성의 정리 (*incompleteness theorem*) 라 한다.
    - 예를 들어 정수 무한대(\\(\infty\\))와 실수 무한대(\\(\infty\\))는 둘 다 무한대이지만 실수 무한대가 더 큰 범위.
        - 이러한 성질을 이용하면 카운터 예제를 만들어낼 수 있다고 한다. (정확한 내용은 잘 모르겠다.)
- 켐브릿지 대학을 갓 졸업한 앨런 튜링은 졸업 후 맥스 뉴먼의 강의에서 이를 증명하는 리뷰 수업을 듣게 된다.
    - 그리고 곧바로 튜링은 자신만의 방식으로 괴텔의 논리를 증명하고자 했다.
- 튜링이 증명한 방식
    - '기계적인 방식'이 무엇인지 정의하고 이를 구현하기 위한 보편 기계를 정의한다. (오로지 5개의 부품으로)
    - 그리고 이 기계가 충분히 보편적이라는 것을 증명한다.
        - 여러가지 문제들을 제시하고 공통의 풀이 방식으로 해결한다.
    - 마지막으로 이 기계로도 풀 수 없는 수학적 문제들을 제시하여 괴텔의 증명을 확인한다.
        - 예를 들어 앞서 설명한 카운터 예제 등은 보편 기계에서는 종료(halt)에 도달하지 못하는 상태가 되어 버린다.
- 그런데 사람들은 정작 보편 기계에 관심을 가지게 됨.
    - 생각보다 현실적인 문제들을 해결할 수 있는 수단을 제공하고 있다.
    - 튜링은 이 머신에 *A-Machine* 이란 이름을 붙였는데 이후 튜링 머신이 된다.
    - 물론 튜링이 제시한 보편 기계는 당시에는 실제 동작하는 기계라기보단 개념에 가까웠음.
    - 사실 튜링 머신은 우리가 알고 있는 오토마타의 일종일 뿐이다.
    
- **튜링 머신 구성 요소**
    - 테이프 (tape) : 일정한 크기의 Cell 로 나뉘어 있는 종이 테이프로 무한하다. (사실은 무한하다는 것 자체가 말이 안되긴 한다.)
    - 헤드 (head) : 종이 테이프의 특정한 셀을 읽을 수 있는 헤드로 이동이 가능. (or 헤드가 고정되고 테이프가 움직여도 된다.)
    - 상태 기록기 (state register) : 현재 튜링 머신의 상태를 저장
        - start state : 시작 상태 (상태 기록기가 초기화된 상태)
        - halt state : 작업 수행이 종료된 상태
    - 액션 테이블 : 특정 상태에서 특적 기호를 읽었을 때 수행해야 할 행동을 지시하는 테이블
        - (예) 종이 테이프의 기호를 고치거나 지울 수 있다.
        - (예) 헤드를 오른쪽 또는 왼쪽으로 한칸 움직이거나 그냥 그 자리에 머문다.
        - (예) 상태 정보를 갱신한다. 혹은 그 상태 그대로 머물 수도 있다.
        - 테이프를 제외하고는 튜링 머신에 사용되는 상태와 액션 테이블은 유한한 크기를 가져야 한다.

- 많은 사람들이 취미로 튜링 머신을 구현한다.

![figure.1]({{ site.baseurl }}/images/{{ page.group }}/f01.jpg){:class="center-block" height="200px"}

- 레고로도 만듬.

![figure.2]({{ site.baseurl }}/images/{{ page.group }}/f02.jpg){:class="center-block" height="300px"}

## Turing Machine Example

- 예를 통해 튜링 머신이 어떻게 동작하는지를 살펴보자.
- 아래 그림은 앞서 설명한 튜링 머신 구성 요소들을 그림으로 나타낸것이다.

![figure.3]({{ site.baseurl }}/images/{{ page.group }}/f03.png){:class="center-block" height="250px"}

- 위 튜링 머신은 \\($\\) 문자열을 맨 뒤로 옮기는 작업을 수행하는 튜링 머신이다.
- 이처럼 튜링 머신은 어떤 목적을 해결하기 위한 수단으로 작성된다.
- 이 때 사용되는 요소는 테이프, 헤드, 상태, 액션테이블 등이다.
- 그림에서 보면,
    - 왼쪽 상단은 테이프를 의미한다. 여기에 데이터를 읽고 쓸 수 있다.
    - 붉은 색 박스가 헤드(head)가 된다. 이를 움직여 테이프 위치를 지정할 수 있다.
    - 왼쪽 하단의 박스가 상태(state)를 나타내는 상자이다. 현재 \\(\\{A, B, C\\}\\) 중 하나의 상태에 매핑될 수 있다.
    - 오른쪽이 액션 테이블로 각 상태에서 전이 가능한 행동들이 기술되어 있다.
    - 이 테이블을 어떻게 작성하는가에 따라 우리가 해결해야 할 문제를 처리할 수 있음을 쉽게 알 수 있다.
- 그림만 봐도 대충 느낌이 오는데 그냥 전형적인 오토마타이다.
- 사실 현재 상태가 어떤 상태인지도 어딘가에 저장을 하고 있어야 하는데,
    - 이것도 사실 테이프에 기록 가능하다.
    - 현재 상태를 읽고 쓰는 방식도 마찬가지로 액션 테이블에 정의하고 테이프에 기록하면 된다.
- 감이 좀 올텐데 예를 들면 복사(copy)라던가 산술 연산 등을 오토마타로 구성할 수 있다.
    - 이를 이용하여 실제 동작을 수행하는 튜링 머신을 구축할 수 있다.


## Neural Turing Machine

- 이제부터는 뉴럴 튜링 머신을 살펴볼 것이다.

### Introduction

- 컴퓨터 프로그램은 다음 3가지 요소로 구성됨.
    - 기본 연산 (예를 들어 수학 연산들)
    - 논리 흐름 컨트롤
    - 외장 메모리 (연산의 결과를 쓰고 읽음)
- 튜링 머신은 현재 컴퓨터 구조의 기본 모델이라 생각할 수 있다.
    - 이 구조를 차용하여 신경망 모델로 확장할 것이다.
- \\(RNN\\) 은 딥러닝 모델 중 아주 성공적인 모델 중 하나이고 시간 \\(t\\) 를 기반으로 하고 있음.
    - 사실 이러한 RNN 이 *Turing-Complete* 하다는 사실이 이미 알려져있음. (Siegelmann & Sontag, 1995)
    - 따라서 튜링 머신으로 \\(RNN\\) 을 흉내낼 수 있다.
- 뉴런 튜링 머신이 기존 튜링 머신과 다른 점은 \\(GD\\) (*gradient descent*) 를 사용하여 학습이 가능하다는 점이다.
- 앞으로 뉴럴 튜링 머신 (Neural Turing Machine) 을 줄여 \\(NTM\\) 이라 표기하도록 하자.

- **RNN**
    - \\(HMM\\) 과 비슷하게 *Sequence* 모델이지만 \\(HMM\\) 보다 더 크기가 충분한 메모리 및 연산량을 가짐.
    - 그리고 최근에는 \\(LSTM\\) 이 좋은 성능을 내고 있음. (Hochreiter & Schmidhuber, 1997)
    - 문제는 *vainshing & exploding* 문제를 가지고 있다는 것.
        - 그래서 긴 *Sequence* 를 가지는 문제를 잘 해결하지는 못한다.


### NTM (a.k.a Neural Turing Machine)

- \\(NTM\\) 은 2개의 컴포넌트를 가지고 있음. : 컨트롤러(controller) 와 메모리 (memory)
- 컨트롤러는 입력, 출력 버퍼(벡터)를 통해 메모리와 통신을 한다.
    - \\(NTM\\) 은 기본 튜링 머신과는 다르게 입출력 연산을 통해 직접 메모리에 접근할 수도 있다.
    - 그리고 이때 사용되는 연산의 내부 파라미터(*weights*)를 **헤드** (head)로 취급하게 된다.
- 모든 컴포넌트들은 미분 가능한 함수로 구성된다.
    - 그래야 \\(GD\\) 를 사용할 수 있다.
    - 그리고 "blurry" 라고 정의한 (어떤 영국 밴드가 떠오른다) 읽기, 쓰기 기능으로 메모리와 반응하는 범위를 만들어 낼 수 있다.
        - 보통 일반적인 튜링 머신에서는 주소 찾기 (addressing) 과정으로 이를 처리하게 된다.
- 전체적인 도식은 다음과 같다.

![figure.4]({{ site.baseurl }}/images/{{ page.group }}/f04.png){:class="center-block" height="250px"}

- 그리고 이 때의 controller 는 신경망으로 구성된다.
- 이제 구성 요소들을 하나씩 살펴보도록 하자.

- - -

#### Memory

- 메모리는 그냥 우리가 알고 있는 메모리와 개념이 같다.
    - 가지만 좀 더 간단하게 *Matrix* 로 표현됨.
- 기본 구조는 다음과 같다.

![figure.5]({{ site.baseurl }}/images/{{ page.group }}/f05.png){:class="center-block" height="200px"}

- 한 메모리 내에 의미있는 정보는 블록 (*Block*) 단위로 나누어진다.
- 현재 메모리 크기는 \\(n\\) 이고 블록 크기는 \\(m\\) 으로 정의한다.
    - 읽고 쓰는 메모리 단위는 *Block* 단위가 된다.

- - -

#### Reading

- 이제 메모리 읽기 연산 *read* 를 정의한다.
- 보통의 경우라면 헤더에 메모리 주소를 저장한 뒤 데이터를 바로 읽어내는 과정으로 작성되겠지만 \\(NTM\\)은 (당연히) 다르다.
- 시간 \\(t\\) 일 때 어떤 정보들을 담고있는 상태의 메모리를 \\(M_t\\) 라고 정의힌다.
    - 이 메모리의 크기는 \\(N \times M\\) 으로 정의된다.
- 이제 *weight* 벡터 \\({\bf w}_t\\) 를 추가한다. 이는 시간 \\(t\\) 일때 읽어야할 메모리 주소 헤더를 반환해주는 벡터가 된다.
- 이 때의 \\({\bf w}_t\\)는 다음의 조건을 가진다. (이 때 \\(i\\) 는 메모리 *row* 인덱스를 의미하게 된다.)

$$\sum_i w_t(i) = 1,\;0 \le w_t(i) \le 1,\;\forall i\qquad{(1)}$$

- 블록 크기가 \\(M\\) 인 메모리로부터 실제 읽은 정보를 담는 벡터를 \\({\bf r}\\) 벡터로 정의한다.
- 이 벡터는 인덱스 \\(i\\) 위치의 메모리 블럭 \\(M_t(i)\\) 와 *convex* 결합으로 구성되는 크기 \\(M\\) 의 벡터로 정의된다.

$${\bf r}_t \leftarrow \sum_{i} w_t(i)M_t(i)\qquad{(2)}$$

- 말로 설명하면 어려운데 그림으로 보면 좀 쉽다.

![figure.6]({{ site.baseurl }}/images/{{ page.group }}/f06.png){:class="center-block" height="300px"}

- 그림을 보면 알 수 있듯 \\(w\_t(i)\\) 값이 높은 메모리 블록의 정보들을 많이 반영하여 얻어진 값을 최종 값으로 사용한다.

- - -

# Writing

- \\(LSTM\\) 으로부터 얻은 영감을 *write* 연산에 적용해본다.
    - 즉, 쓰기 작업을 *erase* 와 *add*, 두 개의 작업으로 분리하여 생각할 것이다.
- 시간 \\(t\\) 에 *write* 용 헤드가 출력하는 *weight* 를 \\({\bf w}_t\\) 라 하자.
- 그리고 이 때 삭제를 위해 사용되는 *erase* 벡터 \\({\bf e}_t\\) 도 정의한다.
    - 이 벡터의 값은 \\(0\\) ~ \\(1\\) 사이의 값만 넣을 수 있다. (실수값이다.)

- 현재 time step 의 바로 전 step 의 메모리 상태를 \\(M\_{t-1}\\) 라 한다면 삭제 연산을 다음과 같이 정의한다.
    
$$\tilde{M}_t(i) \leftarrow M_{t-1}(i)\left[1 - w_t(i) {\bf e}_t\right]\qquad{(3)}$$

- 만약 *erase* 벡터 \\({\bf e}\\) 가 모두 \\(1\\) 을 가진다면 결국 \\(w\\) 값 중 큰 값을 가진 메모리가 0에 가까운 값을 가지게 된다. (삭제 효과)
- 역시나 말로 하면 이해가 잘 안되므로 그림으로 살펴보자.
    - 최종 결과는 메모리에 저장됨을 확인하자.

![figure.7]({{ site.baseurl }}/images/{{ page.group }}/f07.png){:class="center-block" height="250px"}

- 이제 *add* 연산을 보도록 하자.
    - 당연히 삭제와 비슷하게 *add* 용 벡터 \\({\bf a}\\) 가 추가된다.

$$M_t(i) \leftarrow \tilde{M}_t(i) + w_t(i){\bf a}_t\qquad{(4)}$$

![figure.8]({{ site.baseurl }}/images/{{ page.group }}/f08.png){:class="center-block" height="250px"}

### Addressing Mechanisms

- 앞서 메모리에 데이터를 읽고 쓰는 과정을 살펴보았다.
- *blurry* 라는 표현이 딱 맞는게 어떤 *weight* 벡터 \\({\bf w}\\) 를 이용하여 적당하게 퍼진 상태로 데이터를 읽고 쓰는 구조가 가능하다.
- 사실 맨 처음에 소개한 튜링 머신 구조를 잘 보면 결국 일련의 작업 과정을 다음과 같은 형태로 반복하는 것밖에 없다.
    - (1) 메모리로부터 데이터 읽기
    - (2) 읽어들인 데이터를 이해한 다음 상황에 맞는 데이터를 생성하여 메모리에 쓰기
    - (3) 지정된 다른 주소로 이동
    
- 앞서 메모리에 대한 읽기와 쓰기를 살펴보았기 때문에 이제 실제 주소를 이동하는 과정을 확인해봐야 한다.
- 그리고 앞서 살펴본 메모리 읽기, 쓰기에서 힌트가 나오기는 했는데 실제 주소를 접근하는데 사용되는 요소는 바로 벡터 \\({\bf w}\\) 이다.
    - 즉, \\({\bf w}\\) 의 값을 변화시켜 주소를 지정하게 된다.
- 이를 어떻게 지정할지를 알아봐야 하는데 메모리 접근 방식과 마찬가지로 아주 일반화된 형태로 \\({\bf w}\\) 값을 만들어낼 수 있는 방법을 제안한다.
    - 힌트를 주자면 이후 목적에 맞게 이러한 \\({\bf w}\\) 값을 학습하게 될 것이다.
- 그리고 사실 이것이 바로 *controller* 의 역할이 된다.

- 실제로 주소 지정을 위한 방법으로 두가지 매커니즘이 존재한다.
    - *content-based addressing*
    - *location-based addressing*
- 여기서 *content-based addressing* 방식은 *Hopfield* 가 제안한 방식(1982년)으로 입력값과 이에 대해 *controller* 가 출력하는 값의 유사도를 비교하는 방식으로 구성된다.
    - 하지만 모든 문제가 *content-based addressing* 로 풀수 있는 것은 아니다.
    - 예를 들어 수치 연산과 관련된 문제들을 보면 두개의 변수 \\(x\\) 와 \\(y\\) 가 있다고 하고 연산 \\(f(x, y)=x \times y\\) 를 정의하자.
    - *controller* 가 이 연산을 수행하고자 하면 두 개의 변수 값을 메모리에 읽어와 결과를 반드시 어느 메모리 공간에 기록해야 한다.
    - 이런 작업은 명확한 곱셈 연산을 수행하는 작업으로 *content* 와 관련된 연산이 아닌 *location* 에만 관련있는 연산이 된다.
        - 이게 바로 이런 작업을 위한 주소 연산 과정을 을 *location-based addressing* 이라 한다.
    - 사실 *content-based addressing* 는 좀 더 일반적인 연산을 내포하게 된다.
        - 따라서 내부적으로 *location-based addressing* 이 가능하게끔 구조를 설계할 수도 있다.
        - 아래 그림을 보도록 하자. (두 기능이 모두 포함된 구조이다.)

![figure.9]({{ site.baseurl }}/images/{{ page.group }}/f09.png){:class="center-block" height="300px"}

- 위의 그림은 \\(NTM\\) 한 step에서 다음 step으로 전환시 찾아 갈 주소 값을 만들 수 있는 일반 식을 도식화한 그림이다.
    - 결국 이전의 메모리 \\(M\_{t-1}\\)와 \\({\bf w\_{t-1}}\\)를 이용하여 위의 일반 연산들을 수행하면 다음 주소를 얻을 수 있도록 하기 위한 \\({\bf w}\_t\\) 를 만들 수 있다는 이야기.
    - 당연히 중간 중간에 들어간 변환 *weight* 는 데이터를 통해 학습을 하게 될 파라미터가 된다.
        - 즉, 사용자가 별도의 controller 연산을 정의하는 것이 아니라 해당 기능을 표현하고 있는 데이터를 밀어넣어 이 *weight* 들을 학습하게 될 것이다.
    - 막연히 생각해보자면 \\(RNN\\) 과 동일한 작업을 하게 될 것인데 \\(RNN\\) 에서는 과거의 정보를 *hidden* 레이어로 넘겼다면 \\(NTM\\) 에서는 메모리에 저장을 하게 될거란 이야기.
    - 게다가 정확한 값을 기억하는 것이 아니라 신경망의 특성을 이용하여 *embedding* 된 정보로 저장을 하게 될 것이다.
    - 각각이 무엇을 의미하는지 알아보도록 하자.
    
    

    


### 참고자료

- [http://www.slideshare.net/yuzurukato/neural-turing-machines-43179669](http://www.slideshare.net/yuzurukato/neural-turing-machines-43179669)
- [http://www.slideshare.net/ckmarkohchang/neural-turing-machine-tutorial-51270912](http://www.slideshare.net/ckmarkohchang/neural-turing-machine-tutorial-51270912)

